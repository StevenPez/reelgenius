# -*- coding: utf-8 -*-
"""senior_design.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OB-cT058S6NuDy9xY9-BpCCPMW6Q1om3
"""

from tensorflow.keras import layers
from tensorflow import keras
import tensorflow as tf

from sklearn.model_selection import train_test_split
from ast import literal_eval

import pandas as pd
import numpy as np

df = pd.read_csv('rt_imdb.csv')

print(df.head())

print(f"There are {len(df)} rows in the dataset.")
total_duplicate_titles = sum(df["movie_title"].duplicated())
print(f"There are {total_duplicate_titles} duplicate titles.")
print(df.columns.to_list())
print(df.head())

# Vector representation of categorical data
from sklearn.feature_extraction import FeatureHasher

def flatten(l):
    return [item for sublist in l for item in sublist]

df['genres']=df['genres'].fillna("Unknown")
df['genres']=df['genres'].str.replace(' ','')
genres = df['genres'].str.split(',')
genrelist=genres.tolist()
genre_hasher = FeatureHasher(n_features=len(set(flatten(genrelist))), input_type='string')
hashed_genres = genre_hasher.transform(genrelist)
hashed_genres = hashed_genres.toarray()
df['genre_vectors']=pd.Series([hashed_genres[i] for i in range(len(df))])


df['directors']=df['directors'].fillna("Unknown")
df['directors']=df['directors'].str.replace(' ','')
directors = df['directors'].str.split(',')
directorlist=directors.tolist()

from collections import Counter

director_counts = Counter(flatten(directorlist))
sorted_directors = sorted(director_counts.items(), key=lambda x: x[1], reverse=True)
k = 1000  # Number of top directors to consider

top_directors = [director for director, count in sorted_directors[:k]]

df['top_directors'] = [[director for director in director_list if director in top_directors] or ['Unknown'] for director_list in directorlist]


director_hasher = FeatureHasher(n_features=1000, input_type='string')
hashed_directors = director_hasher.transform(df['top_directors'])
hashed_directors = hashed_directors.toarray()
df['director_vectors']=pd.Series([hashed_directors[i] for i in range(len(df))])
df=df.drop(columns='directors')
df=df.drop(columns='top_directors')

df['actors']=df['actors'].fillna("Unknown")
df['actors']=df['actors'].str.replace(' ','').str.split(',')
actors = df['actors']
actorlist=actors.tolist()
len(set(flatten(actors)))

actor_counts = Counter(flatten(actorlist))
sorted_actors = sorted(actor_counts.items(), key=lambda x: x[1], reverse=True)
k = 1000  # Number of top actors to consider

top_actors = [actor for actor, count in sorted_actors[:k]]
top_actors.remove('Jr.')

df['top_actors'] = [[actor for actor in actor_list if actor in top_actors] or ['Unknown'] for actor_list in actorlist]

actor_hasher = FeatureHasher(n_features=1000, input_type='string')
hashed_actors = actor_hasher.transform(df['top_actors'])
hashed_actors = hashed_actors.toarray()
df['actor_vectors']=pd.Series([hashed_actors[i] for i in range(len(df))])
df=df.drop(columns='actors')
df=df.drop(columns='top_actors')

import nltk
#nltk.download('punkt')
#nltk.download('stopwords')
#nltk.download('wordnet')
#nltk.download('word_tokenizes')


from nltk.corpus import wordnet
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

stop_words = set(stopwords.words('english'))
#stop_words = ["i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", “you're”, “you’ve”, “you’ll”, “you’d”, "your’, "yours’, "yourself’, "yourselves’, "he’, "him’, "his’, "himself’, "she’, “she’s”, "her’, "hers’, "herself’, "it’, “it’s”, "its’, "itself’, "they’, "them’, "their’, "theirs’, "themselves’, "what’, "which’, "who’, "whom’, "this’, "that’, “that’ll”, "these’, "those’, "am’, "is’, "are’, "was’, "were’, "be’, "been’, "being’, "have’, "has’, "had’, "having’, "do’, "does’, "did’, "doing’, "a’, "an’, "the’, "and’, "but’, "if’, "or’, "because’, "as’, "until’, "while’, "of’, "at’, "by’, "for’, "with’, "about’, "against’, "between’, "into’, "through’, "during’, "before’, "after’, "above’, "below’, "to’, "from’, "up’, "down’, "in’, "out’, "on’, "off’, "over’, "under’, "again’, "further’, "then’, "once’, "here’, "there’, "when’, "where’, "why’, "how’, "all’, "any’, "both’, "each’, "few’, "more’, "most’, "other’, "some’, "such’, "no’, "nor’, "not’, "only’, "own’, "same’, "so’, "than’, "too’, "very’, "s’, "t’, "can’, "will’, "just’, "don’, “don’t”, "should’, “should’ve”, "now’, "d’, "ll’, "m’, "o’, "re’, "ve’, "y’, "ain’, "aren’, “aren’t”, "couldn’, “couldn’t”, "didn’, “didn’t”, "doesn’, “doesn’t”, "hadn’, “hadn’t”, "hasn’, “hasn’t”, "haven’, “haven’t”, "isn’, “isn’t”, "ma’, "mightn’, “mightn’t”, "mustn’, “mustn’t”, "needn’, “needn’t”, "shan’, “shan’t”, "shouldn’, “shouldn’t”, "wasn’, “wasn’t”, "weren’, “weren’t”, "won’, “won’t”, "wouldn’, “wouldn’t”]
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    if pd.isnull(text):
        return ''

    # Lowercasing
    text = text.lower()

    # Tokenization
    tokens = word_tokenize(text)

    # Stopword removal
    tokens = [token for token in tokens if token not in stop_words]

    # Lemmatization
    tokens = [lemmatizer.lemmatize(token) for token in tokens]

    # Join tokens back into a string
    preprocessed_text = ' '.join(tokens)

    return preprocessed_text

# Apply preprocessing to 'movie_info' column
df['preprocessed_movie_info'] = df['movie_info'].apply(preprocess_text)

# Vectorization using TF-IDF
vectorizer = TfidfVectorizer(max_features=20000)
movie_info_vectors = vectorizer.fit_transform(df['preprocessed_movie_info'])

movie_info=movie_info_vectors.toarray()
df['info_vectors']=pd.Series(movie_info[i] for i in range(len(df)))

# Similarity Score

from sklearn.metrics.pairwise import cosine_similarity

# Select the feature vector columns
feature_columns = ['genre_vectors', 'actor_vectors','director_vectors','info_vectors']
#feature_columns = ['genre_vectors', 'actor_vectors','director_vectors','author_vectors','studio_vectors','info_vectors']
# Compute cosine similarity for each feature vector column
for column in feature_columns:
    feature_vectors = np.vstack(df[column].values)  # Get the feature vectors as dense arrays
    similarity_matrix = cosine_similarity(feature_vectors)  # Compute cosine similarity matrix

    # Aggregate similarity values across feature columns
    if 'similarity' not in df.columns:
        df['similarity'] = np.zeros(len(df))

    df['similarity'] += np.sum(similarity_matrix[:, 1:], axis=1)  # Exclude self-similarity values

# Normalize similarity values
max_similarity = df['similarity'].max()
min_similarity = df['similarity'].min()
df['similarity'] = (df['similarity'] - min_similarity) / (max_similarity - min_similarity)

def get_movie_recommendations(movie_title, similarity_column, num_recommendations, movies):
    # Find the index of the movie with the given title
    movie_index = movies[movies['movie_title'] == movie_title].index[0]

    # Get the similarity values for the given movie index from the specified similarity column
    similarity_values = movies[similarity_column].values

    # Get the similarity value of the input movie
    input_similarity_value = similarity_values[movie_index]

    # Calculate the absolute difference between the similarity values and the input similarity value
    similarity_diff = np.abs(similarity_values - input_similarity_value)

    # Sort the indices based on the similarity difference in ascending order
    sorted_indices = np.argsort(similarity_diff)

    # Exclude the movie itself from the recommendations
    sorted_indices = sorted_indices[sorted_indices != movie_index]

    # Get the top 'num_recommendations' movie titles based on the sorted indices
    recommended_movies = movies.loc[sorted_indices[:num_recommendations], 'movie_title']

    return recommended_movies

recommended_movies = get_movie_recommendations('The Mummy', 'similarity', 20, df)
print(recommended_movies)
